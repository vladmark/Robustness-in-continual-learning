# -*- coding: utf-8 -*-
"""Plotting metrics for Robust Features in TinyImgNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2aMAHNAGnWr-MVGthFZQtMC1QXErx9m
"""
import csv
import os
import sys
from torchvision.datasets import ImageFolder
import torchvision
import torch
import csv
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pickle
from collections import defaultdict
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib import gridspec

def obtain_metrics(path: str, no_tasks = 6, suffix = '', prefix = '', big_dataset = False):
  all_metrics = {}
  for task in ( ["whole_dataset"] if big_dataset else [] ) + list(range(no_tasks)):
    if task == "whole_dataset":
      with open(os.path.join(path,'_metrics_big_dset'+suffix), 'rb') as filehandle:
        all_metrics[task] = pickle.load(filehandle)
    else:
      with open(os.path.join(path,prefix+'_metrics_task_'+str(task)+suffix), 'rb') as filehandle:
        all_metrics[task] = pickle.load(filehandle)
  return all_metrics

def plot_in_a_subplot(subplot, metrics: dict, legend = False, title="Losses for given task"):
    """
    Plots training/validation losses.
    :param metrics: dictionar
    """
    subplot.plot(metrics['train_losses'], c='r', label='Train loss') 
    subplot.plot(metrics['test_losses'], c='g', label='Test loss')
    subplot.plot(metrics['train_acc'], c='b', label='Train acc')
    subplot.plot(metrics['test_acc'], c='y', label='Test acc')
    subplot.set_title(title)
    if legend:
      subplot.set_xlabel('Epoch')
      subplot.legend()

def plot_metrics(all_metrics: dict, model_type: str, save_to = None):
  # Create a Figure
  fig = plt.figure()
  # Set up Axes
  no_rows = len(all_metrics.keys())
  print(f"We had the following tasks: {all_metrics.keys()}. So we'll need {no_rows} rows")
  print(f"last task was: {len(all_metrics.keys())-1}, but tasks start indexed from 0, so actually we access it as task {len(all_metrics.keys())-2}")
  no_columns = len(all_metrics[len(all_metrics.keys())-2]['train_losses']) #on pos 0 we have the big dataset, which must be excluded from this; also subtract 1 because of indexing => subtract 2
  print(f"The maximum number of columns we'll need is {no_columns}")
  axes = fig.subplots(no_rows, no_columns) #no rows, no columns, initial plot #axes is a matrix of axes
  for task_no in range(len(all_metrics.keys()) - 1):
    for previous_task_no in range(task_no+1):
      metrics = {'train_losses': all_metrics[task_no]['train_losses'][previous_task_no], #reconstructing dictionary for a single retested task
                'test_losses': all_metrics[task_no]['test_losses'][previous_task_no],
                'train_acc': all_metrics[task_no]['train_acc'][previous_task_no],
                'test_acc': all_metrics[task_no]['test_acc'][previous_task_no]}
      plot_in_a_subplot(axes[task_no][previous_task_no], metrics, 
                        legend = True if task_no == 0 and previous_task_no == 0 else 0, 
                        title = f"Revisiting task {previous_task_no+1} on task {task_no+1}" if previous_task_no < task_no else f"Training task {task_no+1}")

  #finally, take care of training on whole dataset:
  if 'whole_dataset' in all_metrics.keys():
    metrics = {'train_losses': all_metrics['whole_dataset']['train_losses'][0], #reconstructing dictionary
                  'test_losses': all_metrics['whole_dataset']['test_losses'][0],
                  'train_acc': all_metrics['whole_dataset']['train_acc'][0],
                  'test_acc': all_metrics['whole_dataset']['test_acc'][0]}
    plot_in_a_subplot(axes[-1][0], metrics, legend = True, title = "Trainig progress on united dataset of all tasks.")
    
  #plotting
  fig.suptitle("Training evolution for " + model_type)
  fig.set_figheight(25)
  fig.set_figwidth(25)
  fig.tight_layout()
  if save_to == None:
    fig.show()
  else:
    pp = PdfPages(save_to)
    pp.savefig()
    pp.close()

def plot_acc_average_tasks(all_metrics, model_type, no_tasks = None, save_to = None, basepath = "", only_last_epoch = False, save = True):
  if save_to == None:
    save_to = os.path.join(basepath, "savedump", "acc_plots"+model_type+".pdf")
  if no_tasks == None:
    no_tasks = len(all_metrics.keys())
  accuracies = []
  fig = plt.figure()
  if only_last_epoch:
    accuracies = [all_metrics[task]["all_task_averages"][-1] for task in all_metrics.keys()]
    fig.suptitle(f"Evolution of average accuracies for all tasks for {model_type} - only last epoch of training for each task shown.")
    plt.xlabel('task')
  else:
    for task in all_metrics.keys():
      accuracies += all_metrics[task]["all_task_averages"]
    fig.suptitle(f"Evolution of average accuracies for all tasks for {model_type}")
    plt.xlabel('epoch (total)')

  plt.plot(accuracies, c='b', label = model_type)
  plt.ylabel('acc')
  if save:
    pp = PdfPages(save_to)
    pp.savefig()
    pp.close()
  return

def plot_acc_taskwise(all_metrics, model_type, no_tasks = None, save_to = None, basepath = "", save = True):
  """
  param: all_metrics: a dict containing multiple overall metrics for several training routines
  """
  if save_to == None:
    save_to = os.path.join(basepath, "savedump", "acc_plots"+model_type+".pdf")
  if no_tasks == None:
    for k in all_metrics.keys():
      no_tasks = len(all_metrics[k].keys())
      break
  """
  model_type is used only for title of plot
  for each tasks, construct a single plot by unifying (test) accuracies over training for different tasks
  start with task 0 (on first row)
  """
  plt.rcParams['axes.xmargin'] = 0.0
  fig = plt.figure()
  gs = fig.add_gridspec(ncols = no_tasks, nrows = no_tasks)
  # axes = [fig.add_axes([task/no_tasks, (no_tasks - (task+1))/no_tasks, (no_tasks-task)/no_tasks , 1/no_tasks], frameon = False) for task in range(no_tasks)] #dims are [left, bottom, width, height]
  axes = [fig.add_subplot(gs[task, task:], frameon = False) for task in range(no_tasks)]
  print(f"There are {len(axes)} axes to plot.")
  accuracies = dict()
  for key in all_metrics.keys():
    accuracies[key] = []
    for reconstr_task_no in range(no_tasks):
      accuracies[key].append([])
      for task_no in range(reconstr_task_no, no_tasks): #the only tasks that contain the reconstructed task are the ones bigger than or equal to it
        """
        to reconstruct the task accuracy, look at each task and pick the appropriate accuracy, then append it to the reconstructed acc
        """
        accuracies[key][-1] += all_metrics[key][task_no]['test_acc'][reconstr_task_no]

  for reconstr_task_no in range(no_tasks):
    if reconstr_task_no > 3:
      axes[reconstr_task_no].set_title(f"Task {reconstr_task_no+1} evolution.", fontsize = 6, y=0.1, x = 0.66)
    else:
      axes[reconstr_task_no].set_title(f"Task {reconstr_task_no + 1} evolution.", fontsize=6, y=0.8, pad=-14, x=0.75)
    plots = []
    for key in all_metrics.keys():
      color = "m" if "shuffled" in key and "ewc" in key else "r" if "shuffled" in key else "y" if "ewc" in key else "g"
      # locplot, = axes[reconstr_task_no].plot(accuracies[key][reconstr_task_no], c = color)
      locplot, = axes[reconstr_task_no].plot(accuracies[key][reconstr_task_no], c = color)
      plots.append(locplot)
    if reconstr_task_no == 0:
      axes[reconstr_task_no].legend(plots, [key.replace("_", " ") for key in all_metrics.keys()], loc = "upper right", fontsize = 10)
    axes[reconstr_task_no].set_xlabel('', fontsize = 6)
    # axes[reconstr_task_no].tick_params(axis="y",direction="in", pad=-22)
  # fig.suptitle("Accuracy evolution over training for " + model_type, fontsize = 9)
  fig.set_figheight(10)
  fig.set_figwidth(10)
  # fig.tight_layout(pad = 0.3)
  plt.show()
  # plt.subplots_adjust(hspace = 0.05, top = 0.1, bottom = 0.1)
  if save:
    fig.savefig(save_to)


# for model_type in ["LeNet", "ResNet"]:
#   all_metrics = obtain_metrics(basepath+'/'+model_type+'_15_epochs')
#   print(f"It seems like we revisited task 0 when training task 3 for {len(all_metrics[3]['test_losses'][0])} epochs")
#   plot_metrics_for_model(all_metrics, model_type, "plots_"+model_type+".pdf")

###USAGE EXAMPLE
# model_type = "ResNet" + '_15_epochs'
# all_metrics = obtain_metrics(basepath+'savedump/'+model_type, no_tasks = 6)
#
# plot_all_metrics_for_model(all_metrics = all_metrics, model_type = model_type, save_to = "plots_"+model_type+".pdf")
# plot_accuracies_for_model(all_metrics = all_metrics, model_type = model_type, no_tasks = 6)