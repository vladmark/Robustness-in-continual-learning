# -*- coding: utf-8 -*-
"""Plotting metrics for Robust Features in TinyImgNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2aMAHNAGnWr-MVGthFZQtMC1QXErx9m
"""
import csv
import os
import sys
from torchvision.datasets import ImageFolder
import torchvision
import torch
import csv
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pickle
from collections import defaultdict
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib import gridspec

def obtain_metrics(path: str, no_tasks = 6, suffix = '', prefix = '', big_dataset = False):
  all_metrics = {}
  for task in ( ["whole_dataset"] if big_dataset else [] ) + list(range(no_tasks)):
    if task == "whole_dataset":
      with open(os.path.join(path,'_metrics_big_dset'+suffix), 'rb') as filehandle:
        all_metrics[task] = pickle.load(filehandle)
    else:
      with open(os.path.join(path,prefix+'_metrics_task_'+str(task)+suffix), 'rb') as filehandle:
        all_metrics[task] = pickle.load(filehandle)
  return all_metrics

def plot_in_a_subplot(subplot, metrics: dict, legend = False, title="Losses for given task"):
    """
    Plots training/validation losses.
    :param metrics: dictionar
    """
    subplot.plot(metrics['train_losses'], c='r', label='Train loss') 
    subplot.plot(metrics['test_losses'], c='g', label='Test loss')
    subplot.plot(metrics['train_acc'], c='b', label='Train acc')
    subplot.plot(metrics['test_acc'], c='y', label='Test acc')
    subplot.set_title(title)
    if legend:
      subplot.set_xlabel('Epoch')
      subplot.legend()

def plot_metrics(all_metrics: dict, model_type: str, save_to = None):
  # Create a Figure
  fig = plt.figure()
  # Set up Axes
  no_rows = len(all_metrics.keys())
  print(f"We had the following tasks: {all_metrics.keys()}. So we'll need {no_rows} rows")
  print(f"last task was: {len(all_metrics.keys())-1}, but tasks start indexed from 0, so actually we access it as task {len(all_metrics.keys())-2}")
  no_columns = len(all_metrics[len(all_metrics.keys())-2]['train_losses']) #on pos 0 we have the big dataset, which must be excluded from this; also subtract 1 because of indexing => subtract 2
  print(f"The maximum number of columns we'll need is {no_columns}")
  axes = fig.subplots(no_rows, no_columns) #no rows, no columns, initial plot #axes is a matrix of axes
  for task_no in range(len(all_metrics.keys()) - 1):
    for previous_task_no in range(task_no+1):
      metrics = {'train_losses': all_metrics[task_no]['train_losses'][previous_task_no], #reconstructing dictionary for a single retested task
                'test_losses': all_metrics[task_no]['test_losses'][previous_task_no],
                'train_acc': all_metrics[task_no]['train_acc'][previous_task_no],
                'test_acc': all_metrics[task_no]['test_acc'][previous_task_no]}
      plot_in_a_subplot(axes[task_no][previous_task_no], metrics, 
                        legend = True if task_no == 0 and previous_task_no == 0 else 0, 
                        title = f"Revisiting task {previous_task_no+1} on task {task_no+1}" if previous_task_no < task_no else f"Training task {task_no+1}")

  #finally, take care of training on whole dataset:
  if 'whole_dataset' in all_metrics.keys():
    metrics = {'train_losses': all_metrics['whole_dataset']['train_losses'][0], #reconstructing dictionary
                  'test_losses': all_metrics['whole_dataset']['test_losses'][0],
                  'train_acc': all_metrics['whole_dataset']['train_acc'][0],
                  'test_acc': all_metrics['whole_dataset']['test_acc'][0]}
    plot_in_a_subplot(axes[-1][0], metrics, legend = True, title = "Trainig progress on united dataset of all tasks.")
    
  #plotting
  fig.suptitle("Training evolution for " + model_type)
  fig.set_figheight(25)
  fig.set_figwidth(25)
  fig.tight_layout()
  if save_to == None:
    fig.show()
  else:
    pp = PdfPages(save_to)
    pp.savefig()
    pp.close()

def plot_acc_average_tasks(all_metrics, model_type, no_tasks = None, save_to = None, basepath = "", only_last_epoch = False, save = True):
  """
    param: all_metrics: a dict containing multiple overall metrics for several training routines
  """
  if save_to == None:
    save_to = os.path.join(basepath, "savedump", model_type+"acc_averages_all_tasks.pdf")
  if no_tasks == None:
    no_tasks = len(all_metrics.keys())
  accuracies = dict()
  fig = plt.figure()
  if only_last_epoch:
    fig.suptitle(f"Evolution of average accuracies for all tasks for {model_type} - only last epoch of training for each task shown.")
    plt.xlabel('task')
    for key in all_metrics.keys(): #keys represent different training routines
      accuracies[key] = [all_metrics[key][task]["all_task_averages"][-1] for task in all_metrics[key].keys()]
  else:
    fig.suptitle(f"Evolution of average accuracies for all tasks for {model_type}")
    plt.xlabel('epoch (total)')
    for key in all_metrics.keys():  # keys represent different training routines
      accuracies[key] = []
      for task in all_metrics[key].keys():
          accuracies[key] += all_metrics[key][task]["all_task_averages"]

  plots = []
  for key in all_metrics.keys():
    color = "orange" if "shuffled" in key and "specdec" in key else "cyan" if "specdec" in key \
      else "m" if "shuffled" in key and "ewc" in key else "r" if "shuffled" in key else "y" if "ewc" in key else "g"
    locplot, = plt.plot(accuracies[key], c = color, label = model_type, linewidth = 2)
    plots.append(locplot)
  plt.legend(plots, [key.replace("_", " ") for key in all_metrics.keys()], loc="upper right",
                                fontsize=15)
  plt.ylabel('acc')
  plt.show()
  if save:
    fig.savefig(save_to)
  return

def plot_acc_taskwise(all_metrics, model_type, no_tasks = None, save_to = None, basepath = "", save = True):
  """
  param: all_metrics: a dict containing multiple overall metrics for several training routines
  """
  if save_to == None:
    save_to = os.path.join(basepath, "savedump", "acc_plots"+model_type+".pdf")
  if no_tasks == None:
    for k in all_metrics.keys():
      no_tasks = len(all_metrics[k].keys())
      break
  """
  model_type is used only for title of plot
  for each tasks, construct a single plot by unifying (test) accuracies over training for different tasks
  start with task 0 (on first row)
  """
  import matplotlib.pyplot as plt
  plt.rcParams['axes.xmargin'] = 0.0
  fig = plt.figure()
  gs = fig.add_gridspec(ncols = no_tasks, nrows = no_tasks)
  # axes = [fig.add_axes([task/no_tasks, (no_tasks - (task+1))/no_tasks, (no_tasks-task)/no_tasks , 1/no_tasks], frameon = False) for task in range(no_tasks)] #dims are [left, bottom, width, height]
  axes = [fig.add_subplot(gs[task, task:], frameon = False) for task in range(no_tasks)]
  print(f"There are {len(axes)} axes to plot.")
  accuracies = dict()
  for key in all_metrics.keys():
    accuracies[key] = []
    for reconstr_task_no in range(no_tasks):
      accuracies[key].append([])
      for task_no in range(reconstr_task_no, no_tasks): #the only tasks that contain the reconstructed task are the ones bigger than or equal to it
        """
        to reconstruct the task accuracy, look at each task and pick the appropriate accuracy, then append it to the reconstructed acc
        """
        accuracies[key][-1] += all_metrics[key][task_no]['test_acc'][reconstr_task_no]

  for reconstr_task_no in range(no_tasks):
    if reconstr_task_no > 3:
      axes[reconstr_task_no].set_title(f"Task {reconstr_task_no+1} evolution.", fontsize = 6, y=0.1, x = 0.66)
    else:
      axes[reconstr_task_no].set_title(f"Task {reconstr_task_no + 1} evolution.", fontsize=6, y=0.8, pad=-14, x=0.75)
    plots = []
    for key in all_metrics.keys():
      color = "orange" if "shuffled" in key and "specdec" in key else "cyan" if "specdec" in key \
        else "m" if "shuffled" in key and "ewc" in key else "r" if "shuffled" in key else "y" if "ewc" in key else "g"
      # locplot, = axes[reconstr_task_no].plot(accuracies[key][reconstr_task_no], c = color)
      locplot, = axes[reconstr_task_no].plot(accuracies[key][reconstr_task_no], c = color)
      plots.append(locplot)
    if reconstr_task_no == 0:
      axes[reconstr_task_no].legend(plots, [key.replace("_", " ") for key in all_metrics.keys()], loc = "upper right", fontsize = 10)
    axes[reconstr_task_no].set_xlabel('', fontsize = 6)
    # axes[reconstr_task_no].tick_params(axis="y",direction="in", pad=-22)
  # fig.suptitle("Accuracy evolution over training for " + model_type, fontsize = 9)
  fig.set_figheight(10)
  fig.set_figwidth(10)
  # fig.tight_layout(pad = 0.3)
  plt.show()
  # plt.subplots_adjust(hspace = 0.05, top = 0.1, bottom = 0.1)
  if save:
    fig.savefig(save_to)


# for model_type in ["LeNet", "ResNet"]:
#   all_metrics = obtain_metrics(basepath+'/'+model_type+'_15_epochs')
#   print(f"It seems like we revisited task 0 when training task 3 for {len(all_metrics[3]['test_losses'][0])} epochs")
#   plot_metrics_for_model(all_metrics, model_type, "plots_"+model_type+".pdf")

###USAGE EXAMPLE
# model_type = "ResNet" + '_15_epochs'
# all_metrics = obtain_metrics(basepath+'savedump/'+model_type, no_tasks = 6)
#
# plot_all_metrics_for_model(all_metrics = all_metrics, model_type = model_type, save_to = "plots_"+model_type+".pdf")
# plot_accuracies_for_model(all_metrics = all_metrics, model_type = model_type, no_tasks = 6)


def plot_train_rout_conf_entr(all_metrics, model_type, no_tasks = None, save_to = None, basepath = "", save = True):
  """
  param: all_metrics: a dict containing metrics for just one training routine. its keys are task numbers. the value for each task number is a dict with (relevant) keys "test_confusion" and "test_entropies"
  """
  import torch
  if save_to == None:
    save_to = os.path.join(basepath, "savedump", "confusion_plots"+model_type+".pdf")
  if no_tasks == None:
    no_tasks = len(all_metrics.keys())
  import matplotlib.pyplot as plt
  plt.rcParams['axes.xmargin'] = 0.0
  fig = plt.figure()
  gs = fig.add_gridspec(ncols=no_tasks, nrows=no_tasks)
  axes = [fig.add_subplot(gs[task, task:], frameon=False) for task in range(no_tasks)]
  # for elt in all_metrics[0]['test_entropies'][0][0][0]:
  #   print(f"elt {elt} of type {type(elt)}")
  """
    a little fix: unfortunately key for test confusions was sometimes 'test_confusion' and sometimes 'test_confusions'
  """
  conf_key = 'test_confusions' if 'test_confusions' in all_metrics[0].keys() else 'test_confusion' if 'test_confusion' in all_metrics[0].keys() else 'test_confusion'
  no_classes = all_metrics[0][conf_key][0][0].shape[0]
  """
  confusions: first level keys: tasks ; second level keys: classes. confusions[task][class] is a list of accuracies on a class for task over all epchs
  similarly for entropies; entropies[task][class] is a list of average over logits predicted rightly for class over task over all epochs
  
  entropy double list:  e[i][j] = tensor containing logits on class i when it was classified as class j
  """
  confusions = []
  entropies = []

  for reconstr_task_no in range(no_tasks):
    confusions.append([])
    entropies.append([])
    for cls in range(no_classes):
      confusions[-1].append([])
      entropies[-1].append([])
      for task_no in range(reconstr_task_no,
                           no_tasks):  # the only tasks that contain the reconstructed task are the ones bigger than or equal to it
          """
          a little fix: unfortunately key for test confusions was sometimes 'test_confusion' and sometimes 'test_confusions'
          """
          conf_key = 'test_confusions' if 'test_confusions' in all_metrics[task_no].keys() else 'test_confusion' if 'test_confusion' in all_metrics[task_no].keys() else 'test_confusion'
          confusions[reconstr_task_no][cls] += [matr[cls, cls]/matr[cls, :].sum() if matr[cls, cls] != 0 else 0 for matr in all_metrics[task_no][conf_key][reconstr_task_no]]
          # print(confusions[reconstr_task_no][cls])
          # there was a mistake in creation of entr_list; each entr_list[cls][cls] should be a tensor, but instead it is always a list with one element

          # for elt in all_metrics[task_no]['test_entropies'][reconstr_task_no][cls][cls]: #TEST
          #   print(f"elt {elt} of type {type(elt)}")

          entropies[reconstr_task_no][cls] += [torch.mean(entr_list[cls][cls][0])
                                               if isinstance(entr_list[cls][cls], list) and entr_list[cls][cls]
                                               else entr_list[cls][cls] if isinstance(entr_list[cls][cls], torch.Tensor)
                                                else 0 for entr_list in all_metrics[task_no]['test_entropies'][reconstr_task_no]]
  colors = ['m', 'g', 'r', 'y', 'b', 'k', 'cyan', 'orange']
  for reconstr_task_no in range(no_tasks):
    if reconstr_task_no > 3:
      axes[reconstr_task_no].set_title(f"Task {reconstr_task_no+1} evolution.", fontsize = 6, y=0.1, x = 0.66)
    else:
      axes[reconstr_task_no].set_title(f"Task {reconstr_task_no + 1} evolution.", fontsize=6, y=0.8, pad=-14, x=0.75)
    plots = []
    for cls in range(no_classes):
      # print(confusions[reconstr_task_no][cls])
      locplot, = axes[reconstr_task_no].plot(confusions[reconstr_task_no][cls], c = colors[cls])
      axes[reconstr_task_no].plot(entropies[reconstr_task_no][cls], c = colors[cls], linestyle = 'dashed')
      plots.append(locplot)
    if reconstr_task_no == 0:
      axes[reconstr_task_no].legend(plots, [f"class {cls}" for cls in range(no_classes)], loc = "upper right", fontsize = 10)
    axes[reconstr_task_no].set_xlabel('', fontsize = 6)
  fig.suptitle(f"{save_to}")
  fig.set_figheight(10)
  fig.set_figwidth(10)
  plt.show()
  if save:
    fig.savefig(save_to)
  return