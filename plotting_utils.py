# -*- coding: utf-8 -*-
"""Plotting metrics for Robust Features in TinyImgNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p2aMAHNAGnWr-MVGthFZQtMC1QXErx9m
"""
import csv
import os
import sys
from torchvision.datasets import ImageFolder
import torchvision
import torch
import csv
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import pickle
from collections import defaultdict
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib import gridspec

def obtain_metrics(path: str, no_tasks = 6, big_dataset = False):
  all_metrics = {}
  for task in ( ["whole_dataset"] if big_dataset else [] ) + list(range(no_tasks)):
    if task == "whole_dataset":
      with open(path+'_metrics_big_dset', 'rb') as filehandle:
        all_metrics[task] = pickle.load(filehandle)
    else:
      with open(path+'_metrics_task_'+str(task), 'rb') as filehandle:
        all_metrics[task] = pickle.load(filehandle)
  return all_metrics

def plot_in_a_subplot(subplot, metrics: dict, legend = False, title="Losses for given task"):
    """
    Plots training/validation losses.
    :param metrics: dictionar
    """
    subplot.plot(metrics['train_losses'], c='r', label='Train loss') 
    subplot.plot(metrics['test_losses'], c='g', label='Test loss')
    subplot.plot(metrics['train_acc'], c='b', label='Train acc')
    subplot.plot(metrics['test_acc'], c='y', label='Test acc')
    subplot.set_title(title)
    if legend:
      subplot.set_xlabel('Epoch')
      subplot.legend()

def plot_metrics_for_model (all_metrics: dict, model_type: str, save_to = None):
  # Create a Figure
  fig = plt.figure()
  # Set up Axes
  no_rows = len(all_metrics.keys())
  print(f"We had the following tasks: {all_metrics.keys()}. So we'll need {no_rows} rows")
  print(f"last task was: {len(all_metrics.keys())-1}, but tasks start indexed from 0, so actually we access it as task {len(all_metrics.keys())-2}")
  no_columns = len(all_metrics[len(all_metrics.keys())-2]['train_losses']) #on pos 0 we have the big dataset, which must be excluded from this; also subtract 1 because of indexing => subtract 2
  print(f"The maximum number of columns we'll need is {no_columns}")
  axes = fig.subplots(no_rows, no_columns) #no rows, no columns, initial plot #axes is a matrix of axes
  for task_no in range(len(all_metrics.keys()) - 1):
    for previous_task_no in range(task_no+1):
      metrics = {'train_losses': all_metrics[task_no]['train_losses'][previous_task_no], #reconstructing dictionary for a single retested task
                'test_losses': all_metrics[task_no]['test_losses'][previous_task_no],
                'train_acc': all_metrics[task_no]['train_acc'][previous_task_no],
                'test_acc': all_metrics[task_no]['test_acc'][previous_task_no]}
      plot_in_a_subplot(axes[task_no][previous_task_no], metrics, 
                        legend = True if task_no == 0 and previous_task_no == 0 else 0, 
                        title = f"Revisiting task {previous_task_no+1} on task {task_no+1}" if previous_task_no < task_no else f"Training task {task_no+1}")

  #finally, take care of training on whole dataset:
  if 'whole_dataset' in all_metrics.keys():
    metrics = {'train_losses': all_metrics['whole_dataset']['train_losses'][0], #reconstructing dictionary
                  'test_losses': all_metrics['whole_dataset']['test_losses'][0],
                  'train_acc': all_metrics['whole_dataset']['train_acc'][0],
                  'test_acc': all_metrics['whole_dataset']['test_acc'][0]}
    plot_in_a_subplot(axes[-1][0], metrics, legend = True, title = "Trainig progress on united dataset of all tasks.")
    
  #plotting
  fig.suptitle("Training evolution for " + model_type)
  fig.set_figheight(25)
  fig.set_figwidth(25)
  fig.tight_layout()
  if save_to == None:
    fig.show()
  else:
    pp = PdfPages(save_to)
    pp.savefig()
    pp.close()

def plot_accuracies_for_model(all_metrics, model_type, no_tasks = None, save_to = None, basepath = "./"):
  if save_to == None:
    save_to = basepath+"/savedump/"+"acc_plots"+model_type+".pdf"
  if no_tasks == None:
    no_tasks = len(all_metrics.keys())
  """
  model_type is used only for title of plot
  for each tasks, construct a single plot by unifying (test) accuracies over training for different tasks
  start with task 0 (on first row)
  """
  fig = plt.figure()
  print("got here")
  axes = [plt.axes([task/no_tasks, (task+1)/no_tasks, (no_tasks-task)/no_tasks , 1/no_tasks]) for task in range(no_tasks)] #dims are [left, bottom, width, height]
  print(f"There are {len(axes)} axes to plot.")
  accuracies = []
  for reconstr_task_no in range(no_tasks):
    accuracies.append([])
    for task_no in range(reconstr_task_no, no_tasks): #the only tasks that contain the reconstructed task are the ones bigger than or equal to it
      """
      to reconstruct the task accuracy, look at each task and pick the appropriate accuracy, then append it to the reconstructed acc
      """
      accuracies[-1] += all_metrics[task_no]['test_acc'][reconstr_task_no]
    print(f"setting axes for task {reconstr_task_no}")
    axes[reconstr_task_no].plot(accuracies[-1], c='b', label = model_type)
    axes[reconstr_task_no].set_title(f"Task {reconstr_task_no} evolution.")

  fig.suptitle("Accuracy evolution over training for " + model_type)
  fig.set_figheight(10)
  fig.set_figwidth(10)
  fig.tight_layout()
  plt.show()
  plt.subplots_adjust(hspace = 0.05, top = 0.1, bottom = 0.1)
  pp = PdfPages(save_to)
  pp.savefig()
  pp.close()


# for model_type in ["LeNet", "ResNet"]:
#   all_metrics = obtain_metrics(basepath+'/'+model_type+'_15_epochs')
#   print(f"It seems like we revisited task 0 when training task 3 for {len(all_metrics[3]['test_losses'][0])} epochs")
#   plot_metrics_for_model(all_metrics, model_type, "plots_"+model_type+".pdf")

###USAGE EXAMPLE
# model_type = "ResNet" + '_15_epochs'
# all_metrics = obtain_metrics(basepath+'savedump/'+model_type, no_tasks = 6)
#
# plot_all_metrics_for_model(all_metrics = all_metrics, model_type = model_type, save_to = "plots_"+model_type+".pdf")
# plot_accuracies_for_model(all_metrics = all_metrics, model_type = model_type, no_tasks = 6)